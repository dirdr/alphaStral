{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Ministral-3B on PokÃ©mon Showdown\n",
    "Upload `dataset.jsonl` to the Colab runtime before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q --upgrade \"transformers>=5.0.0.dev0\" trl peft accelerate bitsandbytes \"mistral-common>=1.8.6\"\n!pip install -q git+https://github.com/huggingface/transformers.git"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from huggingface_hub import login\nfrom google.colab import userdata\n\nlogin(token=userdata.get(\"HF_TOKEN\"))"
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom transformers import Mistral3ForConditionalGeneration, AutoTokenizer\n\nMODEL = \"mistralai/Ministral-3-3B-Instruct-2512-BF16\"\n\n# H100 - no quantization needed, full BF16\nmodel = Mistral3ForConditionalGeneration.from_pretrained(\n    MODEL,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nmodel.tie_weights()\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "samples = []\n",
    "with open(\"dataset.jsonl\") as f:\n",
    "    for line in f:\n",
    "        s = json.loads(line)\n",
    "        samples.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": s[\"prompt\"]},\n",
    "                {\"role\": \"assistant\", \"content\": s[\"completion\"]},\n",
    "            ]\n",
    "        })\n",
    "\n",
    "random.shuffle(samples)\n",
    "split = int(len(samples) * 0.95)\n",
    "train_data = Dataset.from_list(samples[:split])\n",
    "val_data = Dataset.from_list(samples[split:])\n",
    "\n",
    "print(f\"Train: {len(train_data)} | Val: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sample(sample):\n",
    "    return {\"text\": tokenizer.apply_chat_template(\n",
    "        sample[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )}\n",
    "\n",
    "train_data = train_data.map(format_sample)\n",
    "val_data = val_data.map(format_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTTrainer, SFTConfig\n\ntrainer = SFTTrainer(\n    model=model,\n    processing_class=tokenizer,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    args=SFTConfig(\n        dataset_text_field=\"text\",\n        max_length=512,\n        packing=False,\n        per_device_train_batch_size=16,\n        gradient_accumulation_steps=1,\n        max_steps=1000,\n        learning_rate=1e-4,\n        bf16=True,\n        logging_steps=50,\n        eval_strategy=\"steps\",\n        eval_steps=200,\n        save_strategy=\"steps\",\n        save_steps=200,\n        output_dir=\"output\",\n        optim=\"adamw_torch\",\n        warmup_steps=50,\n        seed=42,\n    ),\n)\n\ntrainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick inference test\nmodel.eval()\n\nprompt = \"Turn 1. Weather: none. Your pokemon: Garchomp (100/100 HP, healthy) | Type: dragon/ground | Atk: 130 SpA: 80 Spe: 102. Opponent: Kingambit (100/100 HP, healthy) | Type: dark/steel | Def: 100 SpD: 60 Spe: 50. What move do you use?\"\n\nencoded = tokenizer.apply_chat_template(\n    [{\"role\": \"user\", \"content\": prompt}],\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n)\ninput_ids = encoded.input_ids if hasattr(encoded, \"input_ids\") else encoded\ninput_ids = input_ids.to(\"cuda\")\n\nwith torch.no_grad():\n    outputs = model.generate(input_ids, max_new_tokens=32, temperature=0.1, do_sample=True)\nprint(tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Merge LoRA into base model and push full model to HuggingFace Hub\nimport torch\nfrom transformers import Mistral3ForConditionalGeneration, AutoTokenizer\nfrom peft import PeftModel\nfrom google.colab import userdata\nimport os\n\nREPO_NAME = \"mistral-hackaton-2026/ministral-3b-pokemon-showdown\"\nMODEL = \"mistralai/Ministral-3-3B-Instruct-2512-BF16\"\nHF_TOKEN = userdata.get(\"HF_TOKEN\")\n\n# Find latest checkpoint\ncheckpoints = sorted([d for d in os.listdir(\"output\") if d.startswith(\"checkpoint-\")])\nadapter_path = os.path.join(\"output\", checkpoints[-1]) if checkpoints else \"output\"\nprint(f\"Loading adapter from: {adapter_path}\")\n\n# Reload base model in full BF16\nprint(\"Loading base model in BF16...\")\nbase_model = Mistral3ForConditionalGeneration.from_pretrained(\n    MODEL,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nbase_model.tie_weights()\n\n# Load the LoRA adapter from local training output\nprint(\"Loading LoRA adapter...\")\npeft_model = PeftModel.from_pretrained(base_model, adapter_path)\n\n# Merge LoRA weights into the base model\nprint(\"Merging weights...\")\nmerged_model = peft_model.merge_and_unload()\n\n# Push full merged model + tokenizer\nprint(\"Pushing merged model to HuggingFace Hub...\")\nmerged_model.push_to_hub(REPO_NAME, token=HF_TOKEN)\ntokenizer.push_to_hub(REPO_NAME, token=HF_TOKEN)\nprint(\"Done!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}