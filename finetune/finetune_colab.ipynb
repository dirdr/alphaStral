{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Ministral-3B on PokÃ©mon Showdown\n",
    "Upload `dataset.jsonl` to the Colab runtime before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q --upgrade \"transformers>=5.0.0.dev0\" trl peft accelerate bitsandbytes \"mistral-common>=1.8.6\"\n!pip install -q git+https://github.com/huggingface/transformers.git"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from huggingface_hub import login\nfrom google.colab import userdata\n\nlogin(token=userdata.get(\"HF_TOKEN\"))"
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom transformers import Mistral3ForConditionalGeneration, AutoTokenizer, BitsAndBytesConfig\n\nMODEL = \"mistralai/Ministral-3-3B-Instruct-2512-BF16\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nmodel = Mistral3ForConditionalGeneration.from_pretrained(\n    MODEL,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\nmodel.tie_weights()\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "samples = []\n",
    "with open(\"dataset.jsonl\") as f:\n",
    "    for line in f:\n",
    "        s = json.loads(line)\n",
    "        samples.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": s[\"prompt\"]},\n",
    "                {\"role\": \"assistant\", \"content\": s[\"completion\"]},\n",
    "            ]\n",
    "        })\n",
    "\n",
    "random.shuffle(samples)\n",
    "split = int(len(samples) * 0.95)\n",
    "train_data = Dataset.from_list(samples[:split])\n",
    "val_data = Dataset.from_list(samples[split:])\n",
    "\n",
    "print(f\"Train: {len(train_data)} | Val: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sample(sample):\n",
    "    return {\"text\": tokenizer.apply_chat_template(\n",
    "        sample[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )}\n",
    "\n",
    "train_data = train_data.map(format_sample)\n",
    "val_data = val_data.map(format_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTTrainer, SFTConfig\n\ntrainer = SFTTrainer(\n    model=model,\n    processing_class=tokenizer,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    args=SFTConfig(\n        dataset_text_field=\"text\",\n        max_length=512,\n        packing=False,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        max_steps=100,\n        learning_rate=1e-4,\n        bf16=True,\n        logging_steps=10,\n        eval_strategy=\"steps\",\n        eval_steps=50,\n        save_strategy=\"steps\",\n        save_steps=50,\n        output_dir=\"output\",\n        optim=\"paged_adamw_8bit\",\n        warmup_steps=10,\n        seed=42,\n    ),\n)\n\ntrainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick inference test\nmodel.eval()\n\nprompt = \"Turn 1. Weather: none. Your pokemon: Garchomp (100/100 HP, healthy) | Type: dragon/ground | Atk: 130 SpA: 80 Spe: 102. Opponent: Kingambit (100/100 HP, healthy) | Type: dark/steel | Def: 100 SpD: 60 Spe: 50. What move do you use?\"\n\ninputs = tokenizer.apply_chat_template(\n    [{\"role\": \"user\", \"content\": prompt}],\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n).to(\"cuda\")\n\nwith torch.no_grad():\n    outputs = model.generate(inputs, max_new_tokens=32, temperature=0.1, do_sample=True)\nprint(tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Push to HuggingFace Hub\nfrom google.colab import userdata\n\nREPO_NAME = \"ministral-3b-pokemon-showdown\"\n\nmodel.push_to_hub(REPO_NAME, token=userdata.get(\"HF_TOKEN\"))\ntokenizer.push_to_hub(REPO_NAME, token=userdata.get(\"HF_TOKEN\"))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}